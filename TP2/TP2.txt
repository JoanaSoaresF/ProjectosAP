Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. If in doubt you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

Atenção:
- Não edite este ficheiro em programas como Word e afins. Use exclusivamente um editor de texto simples. Em caso de dúvida, use o editor do Spyder.
- Não altere a estrutura deste ficheiro. Preencha as respostas apenas nos espaços respectivos (a seguir à tag R#:)
- Pode adicionar linhas no espaço para as respostas mas as respostas devem ser sucintas e directas.

Linking images and reports/Incluir imagens e relatórios
- You can link a .png or .html file in your answers by typing the name of the file in a separate line. The file must be in the same folder as this TP1.txt file. See the examples below:
- Pode ligar às respostas um ficheiro .png ou .html escrevendo o nome do ficheiro numa linha separada. O ficheiro tem de estar presente na mesma pasta em que está este ficheiro TP1.txt. Por exemplo:

exemplo.png
exemplo.html

PERGUNTAS/QUESTIONS:

Q1: Explain and justify the architecture of your agent, describing the network, input and output, and what experiments you did to choose this particular architecture.
Q1; Explique e justifique a arquitectura do seu agente, descrevendo a rede, os valores de entrada e saída e que experiências fez para escolher esta rede em particular.
R1:
Como o estado do nosso agente consiste numa imagem do board de jogo, o nosso agente tem que conseguir identificar e extrair as features certas para representar o board. Sendo assim usamos uma CNN para implementar o nosso agente, com duas camadas de convulsão e MaxPolling seguidas de três dense layers.
A última camada do agente é uma dense layer com 3 neurónios, correpondendo às três ações possíveis. Esta útlima camada tem ativação linear dado que se trata de um problema de regressão: estamos a tentar aproximar a Q-function para determinar a melhor ação. Assim, a parte de convulsão destina-se a extrair as melhores features do input para serem utilizadas no treino e a parte de dense layers
destina-se a aproximar a Q-function.
Experimentamos várias combinações destas camadas, mas menos camadas dificultavam a aproximação da Q-function, sendo que não obtivemos bons resultados. Sendo assim temos três convolution blocks (que são duas Convolutional layers seguidas de MaxPooling) com 16 e 32 filtros.
Os valores obtidos por estas camadas passam depois pela funcão de flatten antes de seguirem para as dense layers, quatro camadas com 256, 128, 64 e 32 neurónios. Estas camadas têm função de ativação "relu" e os seus pesos são inicializados com o HeUniform initializer.

Por fim, o output corresponde a uma camada composta por 3 neurónios escolhemos a função Huber loss como a nossa função de loss, que é semelhante ao MSE mas não responde tão fortemente a outliers, e o Adam como otimizador.

Executámos variadas experiências, mas esta rede foi a que decidimos ter uma melhor relação custo benefício para o nosso agente.
Experimentamos ainda ter batchnormalizaton entre as camadas, porém os resultados que obtivemos não foram os melhores. Possivelmente porque as imagens de input tem pouca diferença entre elas e talvez a batch normalization esconda essas diferenças de modo a que o treino seja mais difícil.


Q2: Describe the scenario for which you tried to optimize your agent, justifying your choice and describing the experiments you did to select this scenario.
Q2: Descreva o cenário para o qual tentou optimizar o seu agente, justificando a sua escolha e descrevendo que experiências fez para seleccionar este cenário.
R2:
Para este problema tentámos otimizar o agente de modo a que este
    (1) não morra por ir contra as paredes;
    (2) não morra por se tentar comer a si próprio;
    (3) escolha comer maçãs porque lhe trás um maior benefício.

Para cumprir com os nossos objetivos começámos por treinar a rede sem nenhuma política para ver o seu resultado. Como era de esperar não foi o melhor.
Em primeiras tentativas, não implementando nenhuma política e sem usar a relva verificamos que o resultador era que a cobra aprendia rapidamente a não morrer, mas aprendia a andar em círculos, não morrendo mas também não apanhando nenhuma maças, como podemos ver no exemplo ./exemplos/ciclo.gif
Depois incluimos a relva durante o treino, para incentivar a cobra a não fazer ciclos. Porém verificamos que cobra continuava a não tentar apanhar as maças.

Para aumentar o número de exemplos disponíveis para treino em que a cobra apanhava a maça implmentamos uma política nesse sentido, e aumentamos o número de maças de modo a facilitar que a cobra as encontrasse aleatóriamente.
Numa primeira política virava-mos a cabeça da cobra em direção á maçã mais antiga de forma a encaminhar a cobra na direção certa mas como tal maçã poderia estar em qualquer lado do mapa, muitas das vezes viravamos a cobra contra si própria. A fim de reduzir este problema e para diminuir os paços necessários até à comida do fruto passámos a considerar apenas a maçã mais próxima (esta raramente se encontra para trás da cobra visto que a mesma veio de lá, do local mais perto da mesma).
Com a implmentação desta política passamos a observar mais casos de sucesso da cobra a perseguir a maça.


Q3: Explain how you generate the experiences for training your agent. Do not forget to explain the details like the balance between exploitation and exploration, how experiences are selected, what information you store in each experience, how large is the pool of experiences and how frequently are new experiences generated. Justify your choices.
Q3: Explique como gerou as experiências para treinar seu agente. Não se esqueça de explicar os detalhes como o equilíbrio entre exploração aleatória e guiada (exploration e exploitation), como as experiências são selecionadas, que informação guarda em cada experiência, quão grande é o conjunto de experiências e com que frequência novas experiências são geradas. Justifique estas escolhas.
R3:
Para treinar o nosso agente começamos por gerar 1028 experiencias. Iniciamos as mesmas apenas com valores de uma política por forma a ter bastantes exemplos de comer maçãs, mostrar mortes do jogador quando este vai para a maçã ignorando os obstáculos no caminho (o próprio corpo). Após gerar os exemplos iniciais começamos o treino propriamente dito.
Para a exploração temos uma probabilidade decrescente durante o treino, variando de 100% a 5%. Se não for uma ação de exploração iremos pôr em prática o conhecimento adquirido pelo nosso agente usando as suas previsões de qual a melhor ação.
Para garantir que a memória continua com exemplos relevantes da cobra a comer maças, a cada 10 episódios é feito um jogo udando completamente a heurística.

Aquando de selecionar as experiências, nós escolhemos guardar todas visto que o nosso conjunto gerado nos pareceu diversificado o suficiente. Para cada experiência guardamos o estado atual do tabuleiro, a ação tomada, a reward associada a essa ação, o novo estado do tabuleiro e o estado do jogo, se acabou ou não.
Guardamos numa replay memory as últimas 50000 experiências e temos um batch size de 1024 sendo necessário pelo menos 2048 exemplos iniciais (preenchidos seguindo a política implementada).


Q4: Explain how you trained your agent: the algorithm used, the discount factor (gamma), the schedule for generating new experiences and training the agent and other techniques. Justify your choices and explain what experiments you did and which alternatives you tried.
Q4: Explique como treinou o seu agente: o algoritmo utilizado, o factor de desconto (gama), a coordenação da geração de novas experiências e treinao do agente, e outras técnicas que tenha usado. Justifique suas escolhas e explique que alternativas testou e como validou experimentalmente estas escolhas.
R4:

Escolhemos utilizar um algoritmo de q-learning, ou seja, o agente vai aprender a q-function para os diferentes estados do jogo. Esta função é o resultado esperado, a reward esperada para a melhor ação que podemos tomar tendo em conta o estado atual (sendo o estado seguinte consideracom com um desconto de gamma). Assim o agente terá de aprender esta função de forma a tomar a melhor decisão, decidir pela ação cuja reward esperada é mais elevada. Nós escolhemos aplicar um gamma de 0.4 que se demonstrou como a melhor opção durante as nossas execussões.
O nosso agente é treinado a cada 6 passos, e o nosso base model é atualizado a cada 100 passos. Estes valores não foram inteiramente aleatórios, foram baseados em exemplos das aulas assim como modificados por experimentação, achamos que o número de passos a que o nosso agente seria treinado ser 6 revelou um melhor equilíbrio entre estes dois valores, e tal fez-se notar durante o treino.

Q5: Discuss this problem and your solution, describing the agent's performance (score, survival, etc.), which aspects of the game were most difficult for the agent to learn, how you tried to remedy these difficulties and what ideas you might have tried if you had more time.
Q5: Discuta este problema e a sua solução, descrevendo o desempenho do agente (pontuação, sobrevivência, etc), que aspectos do jogo foram mais difíceis de aprender para o agente, como tentou colmatar essas dificuldades e que ideias ficaram que poderia experimentar se tivesse mais tempo.
R5:
Entendemos que a nossa solução não teve os melhores resultados, embora durante o treino os nossos resultados fossem aceitáveis e até considerados bons, quando avaliámos a performance da rede num jogo ela deixava bastante a desejar. Enquanto que em treino o número de passos era de média em torno de 30 (verificando-se muitos acima de 100), na execussão com a rede treinada poucos foram os jogos em que o agente realizou 20 passos. Em relação à pontuação, embora em treino fosse habitual valores superiores a 20 (chegando em muitos episódios, não corridos pela política, a 50), quando realizamos a avaliação da rede raros foram os jogos em que saíssemos de 0.
Assim, de início conseguimos encontrar redes que não morriam, no entanto, estas também não procuravam maçãs ou melhores rewards, a fim de resolver este problema procurámos a implementação de uma política que procurasse aumentar a pontuação, o que correu bem no treino mas teve resultados menos significativos durante a execussão, dificultando também a aprendizagem de sobrevivência.
Com mais tempo também gostariamos de treinar com o tabuleiro completo, em que a área de foco referida anteriormente traria maiores vantagens, e treinar durante mais tempo, para a agente ter uma maior variedade de exemplos para aprendizagem.

Como conclusão achamos que deveríamos implementar algum tipo de filtro que facilite o foco do agente em elementos proximos de si, sejam estes elementos positivos como maçãs ou relva, sejam eles o próprio corpo ou uma parede, elementos a evitar. Talvez desta forma, e com este foco o agente fosse mais facilmente treinado e não ficasse tão "confuso" com toda a informação que recebe.
Antes de implementarmos a política verificavamos no treino uma melhor aprendizagem da cobra para o objetivo de não morrer, o que se pode ver na evolução crescente do número de passos na imagem ./examples/steps.png . Também mostrou resultados algo favorávos por haver uma evolução positiva das recompensas conseguidas (isto porque o número de maças durante o treino era elecado, e era fácil encontrar maças aleatóriamente), tal pode ser observado na imagem ./examples/rewards.png