Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. If in doubt you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

Atenção:
- Não edite este ficheiro em programas como Word e afins. Use exclusivamente um editor de texto simples. Em caso de dúvida, use o editor do Spyder.
- Não altere a estrutura deste ficheiro. Preencha as respostas apenas nos espaços respectivos (a seguir à tag R#:)
- Pode adicionar linhas no espaço para as respostas mas as respostas devem ser sucintas e directas.

Linking images and reports/Incluir imagens e relatórios
- You can link a .png or .html file in your answers by typing the name of the file in a separate line. The file must be in the same folder as this TP1.txt file. See the examples below:
- Pode ligar às respostas um ficheiro .png ou .html escrevendo o nome do ficheiro numa linha separada. O ficheiro tem de estar presente na mesma pasta em que está este ficheiro TP1.txt. Por exemplo:

exemplo.png
exemplo.html

PERGUNTAS/QUESTIONS:

Q1: Explain and justify the architecture of your agent, describing the network, input and output, and what experiments you did to choose this particular architecture.
Q1; Explique e justifique a arquitectura do seu agente, descrevendo a rede, os valores de entrada e saída e que experiências fez para escolher esta rede em particular.
R1:
Como o estado do nosso agente consiste numa imagem do board de jogo, o nosso agente tem que conseguir identificar e extrair as features certas para representar o board. Sendo assim usamos uma CNN
para implementar o nosso agente, com três camada de convulsão e MaxPolling seguidas de três dense layers.
A ultima camada do agente é uma dense layer com 3 neurónios, correpondendo às três ações possíveis. Esta útlima camada tem ativação linear dado que se trata de um problema de regressão: estamos a
tentar aproximas a Q-function para determinar a melhor ação. Assim, a parte de convulsão destina-se a extrair as melhores features do input para serem utilizadas no treino e a parte de dense layers
destina-se a aproximar a Q-function.
Experimentamos várias combinações destas camadas mas menos camadas dificultavam a aproximação da Q-function, sendo que não obtivemos bons resultados. Sendo assim temos três convolution blocks (que são
duas Convolutional layers seguidas de MaxPooling) com 64, 32 e 16 filtros.
Os valores obtidos por estas camadas passam depois pela funcão de flatten antes de seguirem para as dense layers, três camadas com 128, 64 e 32 neurónios. Estas camadas têm função de ativação "relu" e
os seus pesos são inicializados com o HeUniform initializer.

Por fim, o output corresponde a uma camada composta por 3 neurónios com uma função de ativação linear, devido ao problema ser identificado como um de regressão linear, escolhemos a função Huber loss
como a nossa função de loss e o Adam como otimizador.

Executámos variadas experiências, mas esta rede foi a que decidimos ter uma melhor relação custo benefício para o nosso agente.
Experimentamos ainda ter batchnormalizaton entre as camadas, porém os resultados que obtivemos não foram os melhores. Possivelmente porque as imagens de input tem pouca diferença entre elas e talvez a batch
normalization esconda essas diferenças de modo a que o treino seja mais difícil. //NOTE


Q2: Describe the scenario for which you tried to optimize your agent, justifying your choice and describing the experiments you did to select this scenario.
Q2: Descreva o cenário para o qual tentou optimizar o seu agente, justificando a sua escolha e descrevendo que experiências fez para seleccionar este cenário.
R2:
Para este problema tentámos otimizar o agente de modo a que este
    (1) não morra por ir contra as paredes;
    (2) não morra por se tentar comer a si próprio;
    (3) escolha comer maçãs porque lhe trás um maior benefício.

Para cumprir com os nossos objetivos começámos por treinar a rede sem nenhuma política para ver o seu resultado. Como era de esperar não foi o melhor, mas insistimos modificando o nosso código até ao momento.
Adicionando e removendo camadas, alterando políticas de recompensa, experimentaando com e sem relva e decidimos também aumentar o número de macãs de forma a aumentar a possíbilidade de um movimento aleatório
encontrar alguma.
Quando encontrámos uma execussão que a cobra girava em torno de si própria introduzimos a primeira política. Nesta primeira política virava-mos a cabeça da cobra em direção á maçã mais antiga de forma a encaminhar
a cobra na direção certa mas como tal maçã poderia estar em qualquer lado do mapa, muitas das vezes viravamos a cobra contra si própria. A fim de reduzir este problema e para diminuir os paços necessários até
à comida do fruto passámos a considerar apenas a maçã mais próxima (esta raramente se encontra para trás da cobra visto que a mesma veio de lá, do local mais perto da mesma).


Q3: Explain how you generate the experiences for training your agent. Do not forget to explain the details like the balance between exploitation and exploration, how experiences are selected, what information you store in each experience, how large is the pool of experiences and how frequently are new experiences generated. Justify your choices.
Q3: Explique como gerou as experiências para treinar seu agente. Não se esqueça de explicar os detalhes como o equilíbrio entre exploração aleatória e guiada (exploration e exploitation), como as experiências são selecionadas, que informação guarda em cada experiência, quão grande é o conjunto de experiências e com que frequência novas experiências são geradas. Justifique estas escolhas.
R3:
Para treinar o nosso agente gerámos 1028 experiencias. Iniciamos as mesmas apenas com valores de uma política por forma a ter bastantes exemplos de comer maçãs, mostrar mortes do jogador quando este vai para a maçã
ignorando os obstáculos no caminho (o próprio corpo). Depois de gerar os exemplos iniciais começamos o treino propriamente dito. Começamos com uma probabilidade de 20% de seguir a política e dentro dos restantes 80%
temos uma probabilidade decrescente durante o treino, variando de 100% a 15%, de seguir uma heuristica aleatória, de exploração (P(exploration) = 0.8 * x, 15 <= x <= 100). Por fim, se a ação não for de encontro
a nenhuma das eurísticas anteriores iremos por em prática o conhecimento adquirido pelo nosso agente (P(exploitation) = 0.8 * (1-x), 15 <= x <= 100).
Durante o treino e porque consideramos insuficiente que o agente siga a política apenas com esta probabilidade inserimos uma nova condição.





Q4: Explain how you trained your agent: the algorithm used, the discount factor (gamma), the schedule for generating new experiences and training the agent and other techniques. Justify your choices and explain what experiments you did and which alternatives you tried.
Q4: Explique como treinou o seu agente: o algoritmo utilizado, o factor de desconto (gama), a coordenação da geração de novas experiências e treinao do agente, e outras técnicas que tenha usado. Justifique suas escolhas e explique que alternativas testou e como validou experimentalmente estas escolhas.
R4:

Q5: Discuss this problem and your solution, describing the agent's performance (score, survival, etc.), which aspects of the game were most difficult for the agent to learn, how you tried to remedy these difficulties and what ideas you might have tried if you had more time.
Q5: Discuta este problema e a sua solução, descrevendo o desempenho do agente (pontuação, sobrevivência, etc), que aspectos do jogo foram mais difíceis de aprender para o agente, como tentou colmatar essas dificuldades e que ideias ficaram que poderia experimentar se tivesse mais tempo.
R5:
